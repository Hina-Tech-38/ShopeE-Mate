1. DATA INTEGRATION OF ALL 3 PLATFORM INTO ONE DATASET
________________________________________
LOADING ALL 3 PLATFORM DATASET
________________________________________
[ ] import pandas as pd
#FOR AMAZON
amazon_data = pd.read_csv("/content/am1_data.csv")
print("Amazon Data:")
##print(amazon_data.head())
amazon_data.head()

[ ]
#FOR FLIPKART
flipkart_data = pd.read_csv("/content/flp1_data.csv")
print("\nFlipkart Data:")
flipkart_data.head()
________________________________________
[ ]
#FOR JIOMART
jiomart_data = pd.read_csv("/content/jm1_data.csv")
print("\nJioMart Data:")
jiomart_data.head()
Show hidden output
________________________________________
NORMALIZING COLUMNS
________________________________________
[ ]
#

import pandas as pd

# Load the datasets
amazon_data = pd.read_csv("/content/am1_data.csv")
flipkart_data = pd.read_csv("/content/flp1_data.csv")
jiomart_data = pd.read_csv("/content/jm1_data.csv")

# Min-Max Normalization Function
def min_max_normalize(df, columns):
    normalized_df = df.copy()
    for column in columns:
        min_value = normalized_df[column].min()
        max_value = normalized_df[column].max()
        normalized_df[column] = (normalized_df[column] - min_value) / (max_value - min_value)
    return normalized_df

# Specify columns to normalize
columns_to_normalize = ['ratings', 'price', 'discount']  # Adjust this list based on your dataset

# Normalize the data
normalized_amazon_data = min_max_normalize(amazon_data, columns_to_normalize)
normalized_flipkart_data = min_max_normalize(flipkart_data, columns_to_normalize)
normalized_jiomart_data = min_max_normalize(jiomart_data, columns_to_normalize)

# Display the normalized data
print("Normalized Amazon Data:")
print(normalized_amazon_data.head())
print("\nNormalized Flipkart Data:")
print(normalized_flipkart_data.head())
print("\nNormalized JioMart Data:")
print(normalized_jiomart_data.head())
________________________________________
COMBINING THE DATASET
________________________________________
[ ]
combined_data = pd.concat([amazon_data, flipkart_data, jiomart_data])
combined_data.to_csv("combined_data.csv", index=False)
________________________________________
[ ]
#loading the combined data as variable training data
training_data=pd.read_csv("/content/combined_data (1).csv")
training_data.head()
________________________________________
keyboard_arrow_down
2. DATA EXPLORATION - (Checking columns and rows etc)
________________________________________
[ ]
training_data.columns
Index(['product_name', 'product_description', 'product_URL', 'Image',
       'ratings', 'review_count', 'discounted_offer', 'original_price',
       'category', 'niche', 'platform_name', 'Unnamed: 11'],
      dtype='object')
________________________________________
[ ]
training_data['product_description']
________________________________________
[ ]
training_data.shape
(3707, 12)
________________________________________
[ ]
training_data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3707 entries, 0 to 3706
Data columns (total 12 columns):
 #   Column               Non-Null Count  Dtype  
---  ------               --------------  -----  
 0   product_name         3707 non-null   object 
 1   product_description  3705 non-null   object 
 2   product_URL          3705 non-null   object 
 3   Image                3701 non-null   object 
 4   ratings              3654 non-null   float64
 5   review_count         3656 non-null   object 
 6   discounted_offer     3679 non-null   object 
 7   original_price       3560 non-null   object 
 8   category             3705 non-null   object 
 9   niche                3705 non-null   object 
 10  platform_name        3704 non-null   object 
 11  Unnamed: 11          0 non-null      float64
dtypes: float64(2), object(10)
memory usage: 347.7+ KB
________________________________________
[ ]
training_data.describe()
________________________________________
[ ]
training_data.isnull().sum()
________________________________________
keyboard_arrow_down
3. DATA CLEANNING AND PREPROCESSING
________________________________________
Handling missing values.
________________________________________
[ ]
#filling in missing value for ratings with default value 3
training_data['ratings'].fillna(3,inplace=True)

#filling in missing value for reviews count with default value 0
training_data['review_count'].fillna(0,inplace=True)

#filling in missing value for reviews count with default value unknown
training_data['product_name'].fillna('',inplace=True)

#filling in missing value for product url  with default value unknown
training_data['product_URL'].fillna('',inplace=True)

#filling in missing value for product description with default value unknown
training_data['product_description'].fillna('',inplace=True)

#filling in missing value for images with default value unknown
training_data['Image'].fillna('',inplace=True)

#filling in missing value for category with default value unknown
training_data['category'].fillna('',inplace=True)

#filling in missing value for niche with default value unknown
training_data['niche'].fillna('',inplace=True)

#filling in missing value for platform name  with default value unknown
training_data['platform_name'].fillna('',inplace=True)
Show hidden output
________________________________________
[ ]
# Filling  missing values for original price
# Converting 'original_price' to numeric, handling commas and hyphens
training_data["original_price"] = pd.to_numeric(training_data["original_price"].str.replace(',', '').str.replace('-', 'NaN'), errors='coerce')
training_data["original_price"].fillna(training_data["original_price"].median(), inplace=True)

# Converting 'discounted_offer' to numeric if it contains non-numeric values
training_data["discounted_offer"] = pd.to_numeric(training_data["discounted_offer"], errors='coerce')
training_data["discounted_offer"].fillna(training_data["discounted_offer"].mean(), inplace=True)
Show hidden output
________________________________________
[ ]
#rechecking
training_data.isnull().sum()
________________________________________
droping unwanted and duplicates columns
________________________________________
[ ]
# Droping  the column unammed
training_data.drop(columns=['Unnamed: 11'], inplace=True)
________________________________________
[ ]
training_data.columns
training_data.head()
________________________________________
removing duplicate rows and rows with missing values | ALSO product_name and product_description duplicates(keeping 1st)
________________________________________
[ ]
print("Duplicate Rows:",training_data.duplicated().sum())
Duplicate Rows: 23
________________________________________
[ ]
# Drop rows with missing values
training_data.dropna(inplace=True)
________________________________________
[ ]
training_data.head()
________________________________________
Renaming Columns names in shorter way
________________________________________
[ ]
#renaming columns in shorter names
#defing the mapping function to rename them
rename_columns_lists = {
    'product_name': 'name',
    'product_description': 'description',
    'product_URL': 'url',
    'Image': 'image',
    'ratings': 'ratings',
    'review_count': 'reviews count',
    'original_price': 'price',
    'discounted_offer': 'offer',
    'category': 'category',
    'niche': 'niche',
    'platform_name': 'platform',
}
#using rename()
training_data.rename(columns=rename_columns_lists, inplace=True)
________________________________________
[ ]
#rechecking
training_data.columns
Index(['name', 'description', 'url', 'image', 'ratings', 'reviews count',
       'offer', 'price', 'category', 'niche', 'platform'],
      dtype='object')
________________________________________
keyboard_arrow_down
4. EXPLORATORY DATA ANALYSIS [EDA]
________________________________________
[ ]
# Checking data types & missing values
print(training_data.info())
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3707 entries, 0 to 3706
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   name           3707 non-null   object 
 1   description    3707 non-null   object 
 2   url            3707 non-null   object 
 3   image          3707 non-null   object 
 4   ratings        3707 non-null   float64
 5   reviews count  3707 non-null   object 
 6   offer          3707 non-null   float64
 7   price          3707 non-null   float64
 8   category       3707 non-null   object 
 9   niche          3707 non-null   object 
 10  platform       3707 non-null   object 
dtypes: float64(3), object(8)
memory usage: 318.7+ KB
None
________________________________________
[ ]
# Summary statistics
print(training_data.describe())
           ratings          offer          price
count  3707.000000    3707.000000    3707.000000
mean      4.141192    1746.000341   15051.447667
std       0.398391    6158.677356   28044.812471
min       1.000000      44.000000      75.000000
25%       3.900000     422.000000    1199.000000
50%       4.200000    1746.000341    2499.000000
75%       4.400000    1746.000341   16859.500000
max       5.000000  177900.000000  239990.000000
________________________________________
Data Distribution & Visualization
________________________________________
[ ]
import matplotlib.pyplot as plt
import seaborn as sns

# Plot price distribution
plt.figure(figsize=(8,5))
sns.histplot(training_data["price"], bins=50, kde=True)
plt.title("Price Distribution")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()
________________________________________
[ ]
# Plot rating distribution
plt.figure(figsize=(8,5))
sns.histplot(training_data["ratings"], bins=20, kde=True, color="orange")
plt.title("Rating Distribution")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()
________________________________________
[ ]
# Countplot of platforms
plt.figure(figsize=(6,4))
sns.countplot(x="platform", data=training_data, palette="coolwarm", order=training_data["platform"].value_counts().index)
plt.title("Product Count by Platform")
plt.xlabel("E-commerce Platform")
plt.ylabel("Count")
plt.legend("FAJ")
plt.show()
________________________________________
[ ]
# most rated counts
training_data['ratings'].value_counts().plot(kind='bar',color='red')
________________________________________
arrow_upward
arrow_downward
spark
link
comment
settings
delete
more_vert
[ ]
# Most popular items
popular_items = training_data['name'].value_counts().head(5)
popular_items.plot(kind='bar',color='red')
plt.title("Most Popular items")
________________________________________
Correlation Analysis
________________________________________
[ ]
# Compute correlation matrix
corr = training_data[["price", "ratings", "offer"]].corr()

# Heatmap
plt.figure(figsize=(6,5))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()
________________________________________
Price Comparison Across Platforms
________________________________________
[ ]
plt.figure(figsize=(8,5))
sns.boxplot(x="platform", y="price", data=training_data, palette="coolwarm",order=training_data["platform"].value_counts().index)
plt.xlabel("E-commerce Platform")
plt.ylabel("Price")
plt.legend("FAJ")
notch=True
plt.title("Price Comparison Across Platforms")
plt.show()
________________________________________
TAGS CREATIONS
________________________________________
[ ]
#checking columns to select from inorder to create TAGS
training_data.columns
Index(['name', 'description', 'url', 'image', 'ratings', 'reviews count',
       'offer', 'price', 'category', 'niche', 'platform'],
      dtype='object')
________________________________________
[ ]
import spacy
________________________________________
[ ]
import spacy
from spacy.lang.en.stop_words import STOP_WORDS

nlp = spacy.load("en_core_web_sm")

def clean_and_extract_tags(text):
    doc = nlp(text.lower())
    tags = [token.text for token in doc if token.text.isalnum() and token.text not in STOP_WORDS]
    return ', '.join(tags)

columns_to_extract_tags_from = ['category', 'niche', 'description']

for column in columns_to_extract_tags_from:
    training_data[column] = training_data[column].apply(clean_and_extract_tags)
________________________________________
[ ]
# Concatenate the cleaned tags from all relevant columns
training_data['Tags'] = training_data[columns_to_extract_tags_from].apply(lambda row: ', '.join(row), axis=1)
________________________________________
[ ]
training_data.head()
________________________________________
[ ]
# Assuming our DataFrame is named 'training_data'
training_data.to_csv('updated_dataset_with_tags.csv', index=False)

# To download the file to our local machine:
from google.colab import files
files.download('updated_dataset_with_tags.csv')
________________________________________
keyboard_arrow_down
5. MODEL TRAINING { RECOMMENDATION ENGINE}
________________________________________
1st Popularity-Based Filtering
________________________________________
[ ]
# Convert 'ratings' column to numeric before groupby operation
training_data['ratings'] = pd.to_numeric(training_data['ratings'], errors='coerce')

# Perform groupby and calculate mean
average_popularity_based_on_ratings = training_data.groupby(['name','reviews count','niche','image'])['ratings'].mean().reset_index()
top_rated_items = average_popularity_based_on_ratings.sort_values(by='ratings', ascending=False)

popularity_base_recommendation = top_rated_items.head(10)

# Format 'ratings' column to display with one decimal place
popularity_base_recommendation['ratings'] = popularity_base_recommendation['ratings'].apply(lambda x: "{:.1f}".format(x))


# Replace hyphens and commas with empty strings, then any remaining non-numeric with 0, and then convert to numeric
popularity_base_recommendation['reviews count'] = popularity_base_recommendation['reviews count'].str.replace(',', '').str.replace('-', '').str.replace('[^0-9]', '0', regex=True)

# Replace empty strings with '0' before converting to float
popularity_base_recommendation['reviews count'] = popularity_base_recommendation['reviews count'].replace('', '0').astype(float).astype(int)

print("Rating Base Recommendation System: (Trending Products)")
popularity_base_recommendation[['name','ratings','reviews count','niche','image']] = popularity_base_recommendation[['name','ratings','reviews count','niche','image']]
popularity_base_recommendation
________________________________________
2nd Content-Based Filtering
________________________________________
[ ]
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

import os
from scipy.sparse import coo_matrix
________________________________________
[ ]
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix_content = tfidf_vectorizer.fit_transform(training_data['Tags'])
cosine_similarities_content = cosine_similarity(tfidf_matrix_content,tfidf_matrix_content)
________________________________________
[ ]
cosine_similarities_content
array([[1.        , 0.49157671, 0.15903587, ..., 0.        , 0.        ,
        0.        ],
       [0.49157671, 1.        , 0.15044541, ..., 0.        , 0.        ,
        0.        ],
       [0.15903587, 0.15044541, 1.        , ..., 0.        , 0.        ,
        0.        ],
       ...,
       [0.        , 0.        , 0.        , ..., 1.        , 0.36900235,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.36900235, 1.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ]])
________________________________________
[ ]
training_data['name'][1500]
________________________________________
[ ]
from operator import index
item_name = 'Mens Solid 100% Cotton Regular Fit Casual Shirt (Kurta Placket Style)'
item_index = training_data[training_data['name']==item_name]
________________________________________
[ ]
cosine_similarities_content[item_index]
array([], shape=(0, 12, 3707), dtype=float64)
________________________________________
[ ]
similar_items = list(enumerate(cosine_similarities_content[item_index]))
________________________________________
[ ]
similar_items = sorted(similar_items, key=lambda x:x[1], reverse=True)
top_similar_items = similar_items[1:10]

recommended_items_indics = [x[0] for x in top_similar_items]
________________________________________
[ ]
training_data.iloc[recommended_items_indics][['name','ratings','price','platform']]
________________________________________
Creating a function which will Recommend Products for Content Base
________________________________________
[ ]
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def content_based_recommendation_system(training_data, item_name, top_n=10,platform=None):
    # Check if the item name exists in the training data
    if item_name not in training_data['name'].values:
        print(f"Item '{item_name}' not found in the training data.")
        return pd.DataFrame()

    # Create a TF-IDF vectorizer for item descriptions
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')

    # Apply TF-IDF vectorization to item descriptions
    tfidf_matrix_content = tfidf_vectorizer.fit_transform(training_data['Tags'])

    # Calculate cosine similarity between items based on descriptions
    cosine_similarities_content = cosine_similarity(tfidf_matrix_content, tfidf_matrix_content)

    # Find the index of the item
    item_index = training_data[training_data['name'] == item_name].index[0]

    # Get the cosine similarity scores for the item
    similar_items = list(enumerate(cosine_similarities_content[item_index]))

    # Sort similar items by similarity score in descending order
    similar_items = sorted(similar_items, key=lambda x: x[1], reverse=True)

    # Get the top N most similar items (excluding the item itself)
    top_similar_items = similar_items[1:top_n+1]


    # Get the indices of the top similar items
    recommended_item_indices = [x[0] for x in top_similar_items]

    # Get the details of the top similar items
    recommended_items_details = training_data.iloc[recommended_item_indices][['name', 'ratings', 'image','url', 'reviews count','platform']]

    return recommended_items_details
________________________________________
[ ]
# Example: Get content-based recommendations for a specific item
item_name = 'RENEE Face Base BB Cream 7 in 1 with SPF 30 PA+++, Enriched with Hyaluronic Acid, Vitamin C, Hydrates, Nourishes & Smoothens Skin Texture, Sesame 30ml'
content_based_recommended = content_based_recommendation_system(training_data, item_name, top_n=10,platform=None)

content_based_recommended
________________________________________
USER QUERY
________________________________________
[ ]
def query(training_data, user_query, top_n=10, platform=None):
    # Assuming you want to find items similar to the query within the 'nam
    item_name = training_data[training_data['name'].str.contains(user_query, case=False)]['name'].iloc[0]
    recommendations = content_based_recommendation_system(training_data, item_name, top_n=10)

    print(recommendations)
________________________________________
[ ]
item_name = 'ASUS Chromebook Intel Celeron Dual Core N4500 - (4 GB/128 GB EMMC Storage/Chrome OS) CX1500CKA-NJ0413 ...'
recommendations = content_based_recommendation_system(training_data, item_name, top_n=10)
print(recommendations)
                                                   name  ratings  \
1418  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1433  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1456  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1462  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1477  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1500  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1506  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1521  ASUS Chromebook Intel Celeron Dual Core N4500 ...      3.8   
1432  Acer Aspire 3 Intel Celeron Dual Core N4500 - ...      4.0   
1443  Acer Aspire 3 Intel Celeron Dual Core N4500 - ...      3.9   

                                                  image  \
1418  https://www.flipkart.com/asus-chromebook-intel...   
1433  https://www.flipkart.com/asus-chromebook-intel...   
1456  https://www.flipkart.com/asus-chromebook-intel...   
1462  https://www.flipkart.com/asus-chromebook-intel...   
1477  https://www.flipkart.com/asus-chromebook-intel...   
1500  https://www.flipkart.com/asus-chromebook-intel...   
1506  https://www.flipkart.com/asus-chromebook-intel...   
1521  https://www.flipkart.com/asus-chromebook-intel...   
1432  https://www.flipkart.com/acer-aspire-3-intel-c...   
1443  https://www.flipkart.com/acer-aspire-3-intel-c...   

                                                    url reviews count  \
1418  https://rukminim2.flixcart.com/image/312/312/x...          186    
1433  https://rukminim2.flixcart.com/image/312/312/x...          205    
1456  https://rukminim2.flixcart.com/image/312/312/x...          205    
1462  https://rukminim2.flixcart.com/image/312/312/x...          186    
1477  https://rukminim2.flixcart.com/image/312/312/x...          205    
1500  https://rukminim2.flixcart.com/image/312/312/x...          205    
1506  https://rukminim2.flixcart.com/image/312/312/x...          186    
1521  https://rukminim2.flixcart.com/image/312/312/x...          205    
1432  https://rukminim2.flixcart.com/image/312/312/x...           79    
1443  https://rukminim2.flixcart.com/image/312/312/x...          148    

      platform  
1418  FLIPKART  
1433  FLIPKART  
1456  FLIPKART  
1462  FLIPKART  
1477  FLIPKART  
1500  FLIPKART  
1506  FLIPKART  
1521  FLIPKART  
1432  FLIPKART  
1443  FLIPKART  
________________________________________
3rd Hybrid Approach
________________________________________
[ ]
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

def hybrid_recommendation_system(training_data, query, top_n=10, content_weight=0.7, query_type="item"):
    """
    Generates hybrid recommendations, handling both item-based and query-based searches.

    Args:
        training_data (pd.DataFrame): DataFrame containing item data.
        query (str): Either an item name (for item-based search) or a general query (for query-based search).
        top_n (int, optional): Number of recommendations to generate. Defaults to 10.
        content_weight (float, optional): Weight given to content-based filtering. Defaults to 0.7.
        query_type (str, optional): Type of query ("item" or "query"). Defaults to "item".

    Returns:
        pd.DataFrame: DataFrame containing the top N recommended items.
    """

    # 1. Determining item name based on query type
    if query_type == "item":
        item_name = query  # If item-based, the query is the item name
    elif query_type == "query":
        # If query-based, find the most relevant item using the query
        # You might need to adjust this logic based on your data and desired behavior
        item_name = training_data[training_data['name'].str.contains(query, case=False)]['name'].iloc[0]
    else:
        print("Invalid query_type. Please use 'item' or 'query'.")
        return pd.DataFrame()

    # 2. Checking if the item name exists in the training data
    if item_name not in training_data['name'].values:
        print(f"Item '{item_name}' not found in the training data.")
        return pd.DataFrame()

    # 3. Content-Based Filtering (similar as before)
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix_content = tfidf_vectorizer.fit_transform(training_data['Tags'])
    cosine_similarities_content = cosine_similarity(tfidf_matrix_content, tfidf_matrix_content)
    item_index = training_data[training_data['name'] == item_name].index[0]
    similar_items_content = list(enumerate(cosine_similarities_content[item_index]))
    similar_items_content = sorted(similar_items_content, key=lambda x: x[1], reverse=True)

    # 4. Popularity-Based Filtering (similar as before)
    average_popularity_based_on_ratings = training_data.groupby(['name'])['ratings'].mean().reset_index()
    top_rated_items = average_popularity_based_on_ratings.sort_values(by='ratings', ascending=False)

    # 5. Hybrid Score Calculation (similar as before)
    hybrid_scores = {}
    for i, score in similar_items_content[1:]:  # Exclude the item itself
        item_name_i = training_data.iloc[i]['name']
        if item_name_i in top_rated_items['name'].values:
            popularity_score = top_rated_items[top_rated_items['name'] == item_name_i]['ratings'].iloc[0]
            hybrid_scores[i] = content_weight * score + (1 - content_weight) * popularity_score
        else:
            hybrid_scores[i] = content_weight * score

    # 6. Get Top N Recommendations (similar as before)
    sorted_hybrid_scores = dict(sorted(hybrid_scores.items(), key=lambda item: item[1], reverse=True))
    top_items_indices = list(sorted_hybrid_scores.keys())[:top_n]

    recommended_items = training_data.iloc[top_items_indices][['name', 'ratings', 'price', 'image', 'url', 'reviews count', 'platform']]
    return recommended_items

________________________________________
[ ]
# Example usage:
# Item-based search
item_name = 'ASUS Chromebook Intel Celeron Dual Core N4500 - (4 GB/128 GB EMMC Storage/Chrome OS) CX1500CKA-NJ0413 ...'
hybrid_recommendations = hybrid_recommendation_system(training_data, item_name, top_n=10, query_type="item")
hybrid_recommendations
________________________________________
[ ]
# Example usage:

# Query-based search
user_query = "wireless headphones"
recommendations = hybrid_recommendation_system(training_data, user_query, top_n=30, query_type="query")
print("\nQuery-based Recommendations:")
print(recommendations)

Query-based Recommendations:
                                                   name  ratings    price  \
750   pTron Newly Launched Zenbuds Evo X5 34dB ANC T...      4.1   4599.0   
755   VEHOP Wow PODS Truly Wireless in Ear Earbuds w...      5.0   4999.0   
756   VEHOP AIRMAX Screen TWS, Smart Touch Display T...      5.0   2999.0   
790   VEHOP AIRMAX Screen TWS, Smart Touch Display T...      5.0   2999.0   
757   VEHOP Wow PODS Truly Wireless in Ear Earbuds w...      5.0   4999.0   
779   VEHOP Open Ear Clip-On TWS Bluetooth Earbuds w...      5.0   4999.0   
685   VEHOP H585 OWS Open Ear Wireless Earbuds | ENC...      5.0   2999.0   
793   VEHOP BATMANN True Wireless in Ear Earbuds, 45...      5.0   4999.0   
173   Noise Newly Launched Air Buds 6 in-Ear Bluetoo...      5.0   3499.0   
200   Noise Newly Launched Air Buds 6 in-Ear Bluetoo...      5.0   3499.0   
3459   pTron Pulsefit Ace 2.01 inch Full Touch 600 N...      5.0   4299.0   
778   JINX in Ear Earbuds High Bass Clear 4D Surroun...      5.0   4999.0   
742   pTron Zenbuds Evo X2 in-Ear TWS Earbuds with Q...      4.0   3799.0   
751   pTron Zenbuds Evo X2 in-Ear TWS Earbuds with Q...      4.0   3799.0   
780   pTron Bassbuds Bliss TWS In-Ear Earbuds with 4...      4.3   2899.0   
3460   DWISAPTTI i7 Pro Max Smart Watch Series 7 for...      5.0    999.0   
705   pTron Bassbuds Bliss TWS In-Ear Earbuds with 4...      4.3   2899.0   
3227                Apple iPhone 16 Plus 128 GB, Black       5.0  89900.0   
776   pTron Bassbuds Bliss TWS In-Ear Earbuds with 4...      4.3   2899.0   
3226   Nothing CMF Phone 1, 8GB RAM, 128GB ROM, Blac...      5.0  21999.0   
713   pTron Bassbuds Bliss TWS In-Ear Earbuds with 4...      4.3   2899.0   
708   Boult Newly Launched K10 Truly Wireless in Ear...      4.2   2499.0   
3168             DAKWINS Black Navy Slim Fit MEN Jeans       5.0   2299.0   
3592   Apple iPad Pro 7th Gen 2024 33.02 cm (13 inch...      5.0   2499.0   
3167     HABBIT N HOBBY Men Cotton black printed jeans       5.0    799.0   
183   Boult Newly Launched K10 Truly Wireless in Ear...      4.2   2499.0   
202   Boult Newly Launched K10 Truly Wireless in Ear...      4.2   2499.0   
3535   DOMO Slate SLP9 T310 10.1-Inch 1920x1200 IPS ...      5.0  29990.0   
730   pTron Zenbuds Evo X1 Max in-Ear TWS Earbuds, 2...      4.0   3999.0   
681   pTron Bassbuds Duo Pro TWS in-Ear Earbuds with...      4.3   2899.0   

                                                  image  \
750   https://m.media-amazon.com/images/I/51-io3x5rc...   
755   https://m.media-amazon.com/images/I/510x34edxD...   
756   https://m.media-amazon.com/images/I/61lN1OKsRu...   
790   https://m.media-amazon.com/images/I/61TQpOGUTw...   
757   https://m.media-amazon.com/images/I/51SFoCCh1a...   
779   https://m.media-amazon.com/images/I/61U297dEGn...   
685   https://m.media-amazon.com/images/I/616UTny7G7...   
793   https://m.media-amazon.com/images/I/61Zp3MXnwR...   
173   https://m.media-amazon.com/images/I/51ze1PI2CH...   
200   https://m.media-amazon.com/images/I/51ze1PI2CH...   
3459  https://www.jiomart.com/images/product/origina...   
778   https://m.media-amazon.com/images/I/518Cw891MT...   
742   https://m.media-amazon.com/images/I/51h7CCmDsJ...   
751   https://m.media-amazon.com/images/I/51+h3cff9M...   
780   https://m.media-amazon.com/images/I/51qpz1lG5q...   
3460  https://www.jiomart.com/images/product/origina...   
705   https://m.media-amazon.com/images/I/51zLPDG36M...   
3227  https://www.jiomart.com/images/product/origina...   
776   https://m.media-amazon.com/images/I/51PCH0hPEe...   
3226  https://www.jiomart.com/images/product/origina...   
713   https://m.media-amazon.com/images/I/51DJx5VXGX...   
708   https://m.media-amazon.com/images/I/41kK3roaPG...   
3168  https://www.jiomart.com/images/product/origina...   
3592  https://www.jiomart.com/images/product/origina...   
3167  https://www.jiomart.com/images/product/origina...   
183   https://m.media-amazon.com/images/I/41kK3roaPG...   
202   https://m.media-amazon.com/images/I/41kK3roaPG...   
3535  https://www.jiomart.com/images/product/origina...   
730   https://m.media-amazon.com/images/I/51ctYPqgwy...   
681   https://m.media-amazon.com/images/I/51MT31JMyq...   

                                                    url reviews count platform  
750   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           297   AMAZON  
755   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             1   AMAZON  
756   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             2   AMAZON  
790   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             2   AMAZON  
757   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             3   AMAZON  
779   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             2   AMAZON  
685   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             2   AMAZON  
793   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             1   AMAZON  
173   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             2   AMAZON  
200   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             2   AMAZON  
3459  https://www.jiomart.com/p/electronics/ptron-pu...             -  JIOMART  
778   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...             4   AMAZON  
742   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           343   AMAZON  
751   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           344   AMAZON  
780   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           312   AMAZON  
3460  https://www.jiomart.com/p/electronics/dwisaptt...             -  JIOMART  
705   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           312   AMAZON  
3227  https://www.jiomart.com/p/electronics/apple-ip...             -  JIOMART  
776   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           312   AMAZON  
3226  https://www.jiomart.com/p/electronics/nothing-...             -  JIOMART  
713   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           312   AMAZON  
708   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...         2,543   AMAZON  
3168  https://www.jiomart.com/p/fashion/dakwins-blac...             -  JIOMART  
3592  https://www.jiomart.com/p/electronics/apple-ip...             -           
3167  https://www.jiomart.com/p/fashion/habbit-n-hob...             -  JIOMART  
183   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...         2,543   AMAZON  
202   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...         2,543   AMAZON  
3535  https://www.jiomart.com/p/electronics/domo-sla...             -  JIOMART  
730   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           344   AMAZON  
681   https://www.amazon.in/sspa/click?ie=UTF8&spc=M...           312   AMAZON  
________________________________________
#DONE HERE THE END.
________________________________________
